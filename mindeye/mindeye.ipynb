{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6053a83-2259-475e-9e21-201e44217e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using storage path: /home/ri4541@pu.win.princeton.edu/rt_mindeye/rt_all_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye\n"
     ]
    }
   ],
   "source": [
    "# set up main path where everything will be you should download the\n",
    "# hugging face directory described in readme and put it here on the same\n",
    "# server where the data analyzer is run so that the data analyzer code with \n",
    "# the GPU can access these files\n",
    "# You should replace the below path with your location\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    with open('conf/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    storage_path = config['storage_path']\n",
    "    data_path = config['data_path']\n",
    "    derivatives_path = config['derivatives_path']\n",
    "    fsl_path = config['fsl_path']\n",
    "    assert os.path.exists(storage_path), \"The specified data and model storage path does not exist.\"\n",
    "    assert os.path.exists(data_path), \"The specified BOLD path does not exist.\"\n",
    "    assert os.path.exists(derivatives_path), \"The specified derivatives path does not exist.\"\n",
    "    assert os.path.exists(fsl_path), \"The specified FSL path does not exist.\"\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"config.json file not found. Please create it with the required paths.\")\n",
    "\n",
    "print(f\"Using storage path: {storage_path}\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------------------\n",
    "Imports and set up for mindEye\n",
    "-----------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "# print(sys.path)\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# custom functions #\n",
    "import utils_mindeye\n",
    "from models import *\n",
    "import pandas as pd\n",
    "import ants\n",
    "import nilearn\n",
    "import pdb\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4516e788-85cc-42ab-b05a-11bd7207f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir= f\"{storage_path}/cache\"\n",
    "model_name=\"sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0\"\n",
    "subj=1\n",
    "hidden_dim=1024\n",
    "blurry_recon = False\n",
    "n_blocks=4 \n",
    "seq_len = 1\n",
    "\n",
    "import pickle\n",
    "with open(f\"{storage_path}/clip_img_embedder\", \"rb\") as input_file:\n",
    "    clip_img_embedder = pickle.load(input_file)\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12be1838-f387-4cdd-b7cb-217a74501359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "8,835,072 total\n",
      "8,835,072 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "462,195,352 total\n",
      "462,195,352 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "722,060,568 total\n",
      "722,060,552 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "722060552"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "num_voxels = 8627\n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "\n",
    "\n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "\n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "\n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                        clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils_mindeye.count_params(model.ridge)\n",
    "utils_mindeye.count_params(model.backbone)\n",
    "utils_mindeye.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils_mindeye.count_params(model.diffusion_prior)\n",
    "utils_mindeye.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a627d35-3cd5-4cd1-9bb3-c02c0c97f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "# Replace with pre_trained_fine_tuned_model.pth\n",
    "# tag='pretrained_fine-tuned_sliceTimed0.5.pth'\n",
    "# tag='pretrained_fine-tuned_sliceTimed.pth'\n",
    "tag='sub-005_all_task-C_bs24_MST_rishab_MSTsplit_union_mask_finetune_0.pth'\n",
    "outdir = os.path.abspath(f'3t/data/model')\n",
    "\n",
    "# print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "# try:\n",
    "checkpoint = torch.load(outdir+f'/{tag}', map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "del checkpoint\n",
    "# except: # probably ckpt is saved using deepspeed format\n",
    "#     import deepspeed\n",
    "#     state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "#     model.load_state_dict(state_dict, strict=False)\n",
    "#     del state_dict\n",
    "# print(\"ckpt loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bd11f3-6d4d-4ee4-a23b-443afeb5c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "# first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "# first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "with open(f\"{storage_path}/diffusion_engine\", \"rb\") as input_file:\n",
    "    diffusion_engine = pickle.load(input_file)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "    \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "    \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "# f = h5py.File(f'{storage_path}/coco_images_224_float16.hdf5', 'r')\n",
    "# images = f['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cb6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = \"sub-005\"\n",
    "session = \"ses-03\"\n",
    "task = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "func_task_name = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "n_runs = 11\n",
    "\n",
    "ses_list = [session]\n",
    "design_ses_list = [session]\n",
    "    \n",
    "task_name = f\"_task-{task}\" if task != 'study' else ''\n",
    "designdir = f\"{data_path}/events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48586a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 126)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-03.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 532\n",
      "n_runs 11\n",
      "['all_stimuli/unchosen_nsd_1000_images/unchosen_7211_cocoid_59250.png'\n",
      " 'all_stimuli/special515/special_67295.jpg'\n",
      " 'all_stimuli/unchosen_nsd_1000_images/unchosen_5729_cocoid_53029.png'\n",
      " 'all_stimuli/special515/special_70232.jpg']\n",
      "[174.7109683 178.7049172 182.7072832 186.7297016]\n",
      "[0. 0. 0. 0.]\n",
      "(693,)\n"
     ]
    }
   ],
   "source": [
    "data, starts, images, is_new_run, image_names, unique_images, len_unique_images = utils_mindeye.load_design_files(\n",
    "    sub=sub,\n",
    "    session=session,\n",
    "    func_task_name=task,\n",
    "    designdir=designdir,\n",
    "    design_ses_list=design_ses_list\n",
    ")\n",
    "\n",
    "if sub == 'sub-001':\n",
    "    if session == 'ses-01':\n",
    "        assert image_names[0] == 'images/image_686_seed_1.png'\n",
    "    elif session in ('ses-02', 'all'):\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_40840.jpg'\n",
    "    elif session == 'ses-03':\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_69839.jpg'\n",
    "    elif session == 'ses-04':\n",
    "        assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "elif sub == 'sub-003':\n",
    "    assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "\n",
    "unique_images = np.unique(image_names.astype(str))\n",
    "unique_images = unique_images[(unique_images!=\"nan\")]\n",
    "len_unique_images = len(unique_images)\n",
    "print(\"n_runs\",n_runs)\n",
    "\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(unique_images) == 851\n",
    "\n",
    "print(image_names[:4])\n",
    "print(starts[:4])\n",
    "print(is_new_run[:4])\n",
    "\n",
    "image_idx = np.array([])  # contains the unique index of each presented image\n",
    "vox_image_names = np.array([])  # contains the names of the images corresponding to image_idx\n",
    "all_MST_images = dict()\n",
    "for i, im in enumerate(image_names):\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    vox_image_names = np.append(vox_image_names, im)\n",
    "            \n",
    "    image_idx_ = np.where(im==unique_images)[0].item()\n",
    "    image_idx = np.append(image_idx, image_idx_)\n",
    "    \n",
    "    all_MST_images[i] = im\n",
    "    i+=1\n",
    "    \n",
    "image_idx = torch.Tensor(image_idx).long()\n",
    "# for im in new_image_names[MST_images]:\n",
    "#     assert 'MST_pairs' in im\n",
    "# assert len(all_MST_images) == 300\n",
    "\n",
    "unique_MST_images = np.unique(list(all_MST_images.values())) \n",
    "\n",
    "MST_ID = np.array([], dtype=int)\n",
    "\n",
    "vox_idx = np.array([], dtype=int)\n",
    "j=0  # this is a counter keeping track of the remove_random_n used later to index vox based on the removed images; unused otherwise\n",
    "for i, im in enumerate(image_names):  # need unique_MST_images to be defined, so repeating the same loop structure\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    j+=1\n",
    "    curr = np.where(im == unique_MST_images)\n",
    "    # print(curr)\n",
    "    if curr[0].size == 0:\n",
    "        MST_ID = np.append(MST_ID, np.array(len(unique_MST_images)))  # add a value that should be out of range based on the for loop, will index it out later\n",
    "    else:\n",
    "        MST_ID = np.append(MST_ID, curr)\n",
    "        \n",
    "assert len(MST_ID) == len(image_idx)\n",
    "# assert len(np.argwhere(pd.isna(data['current_image']))) + len(np.argwhere(data['current_image'] == 'blank.jpg')) + len(image_idx) == len(data)\n",
    "# MST_ID = torch.tensor(MST_ID[MST_ID != len(unique_MST_images)], dtype=torch.uint8)  # torch.tensor (lowercase) allows dtype kwarg, Tensor (uppercase) is an alias for torch.FloatTensor\n",
    "print(MST_ID.shape)\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(all_MST_images) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d16a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                     | 0/693 [00:00<?, ?it/s]/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 693/693 [00:06<00:00, 105.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images torch.Size([693, 3, 224, 224])\n",
      "len MST_images 693\n",
      "MST_images==True 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "MST_images = []\n",
    "images = None\n",
    "for im_name in tqdm(image_idx):\n",
    "    image_file = f\"{unique_images[im_name]}\"\n",
    "    im = imageio.imread(f\"{data_path}/{image_file}\")\n",
    "    im = torch.Tensor(im / 255).permute(2,0,1)\n",
    "    im = resize_transform(im.unsqueeze(0))\n",
    "    if images is None:\n",
    "        images = im\n",
    "    else:\n",
    "        images = torch.vstack((images, im))\n",
    "    if (\"MST_pairs\" in image_file): # (\"_seed_\" not in unique_images[im_name]) and (unique_images[im_name] != \"blank.jpg\") \n",
    "        MST_images.append(True)\n",
    "    else:\n",
    "        MST_images.append(False)\n",
    "\n",
    "print(\"images\", images.shape)\n",
    "MST_images = np.array(MST_images)\n",
    "print(\"len MST_images\", len(MST_images))\n",
    "if not (sub == 'sub-005' and session == 'ses-06'):\n",
    "    assert len(MST_images[MST_images==True]) == 124\n",
    "print(\"MST_images==True\", len(MST_images[MST_images==True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f692b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs(sub, session, func_task_name, designdir):\n",
    "    \"\"\"Loads design files and processes image pairs for a given session.\"\"\"\n",
    "    _, _, _, _, image_names, unique_images, _ = utils_mindeye.load_design_files(\n",
    "        sub=sub,\n",
    "        session=session,\n",
    "        func_task_name=func_task_name,\n",
    "        designdir=designdir,\n",
    "        design_ses_list=[session]  # Ensure it's a list\n",
    "    )\n",
    "    return utils_mindeye.process_images(image_names, unique_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbffed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 126)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-03.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 532\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_dicts = []\n",
    "for s_idx, s in enumerate(ses_list):\n",
    "    im, vo, _ = get_image_pairs(sub, s, func_task_name, designdir)\n",
    "    assert len(im) == len(vo)\n",
    "    all_dicts.append({k:v for k,v in enumerate(vo)})\n",
    "\n",
    "image_to_indices = defaultdict(lambda: [[] for _ in range(len(ses_list))])\n",
    "for ses_idx, idx_to_name in enumerate(all_dicts):\n",
    "    for idx, name in idx_to_name.items():\n",
    "        image_to_indices[name][ses_idx].append(idx)\n",
    "        \n",
    "image_to_indices = dict(image_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b2474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MST_idx 62\n"
     ]
    }
   ],
   "source": [
    "utils_mindeye.seed_everything(0)\n",
    "MST_idx = [v[0][0] if len(v[0]) > 0 else None for k, v in image_to_indices.items() if 'MST_pairs' in k]\n",
    "# Remove any None values (in case some images don't have repeats)\n",
    "MST_idx = [idx for idx in MST_idx if idx is not None]\n",
    "\n",
    "print(\"MST_idx\", len(MST_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05736bc-c816-49ae-8718-b6c31b412781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "\n",
    "# get the mask and the reference files\n",
    "ndscore_events = [pd.read_csv(f'{data_path}/events/{sub}_{session}_task-{func_task_name}_run-{run+1:02d}_events.tsv', sep = \"\\t\", header = 0) for run in range(n_runs)]  # create a new list of events_df's which will have the trial_type modified to be unique identifiers\n",
    "ndscore_tr_labels = [pd.read_csv(f\"{data_path}/events/{sub}_{session}_task-{func_task_name}_run-{run+1:02d}_tr_labels.csv\") for run in range(n_runs)]\n",
    "tr_length = 1.5\n",
    "mask_img = nib.load(f'{data_path}/{sub}_final_mask.nii.gz')  # nsdgeneral mask in functional space\n",
    "assert sub == 'sub-005'\n",
    "ses1_boldref= f\"{data_path}/sub-005_ses-01_task-C_run-01_space-T1w_boldref.nii.gz\"  # preprocessed boldref from ses-01\n",
    "single_vols_path = f\"{derivatives_path}/vols/{sub}/{session}\"\n",
    "ses3_vol0 = os.path.join(single_vols_path, f\"{sub}_{session}_task-{func_task_name}_run-01_bold_0000.nii.gz\") # first volume (vol0000) of real-time session\n",
    "assert os.path.exists(ses3_vol0)\n",
    "# day2_to_day1_mat =  f\"{storage_path}/day2ref_to_day1ref\"\n",
    "def fast_apply_mask(target=None,mask=None):\n",
    "    return target[np.where(mask == 1)].T\n",
    "ses1_boldref_nib = nib.load(ses1_boldref)\n",
    "union_mask = np.load(f\"{data_path}/union_mask_from_ses-01-02.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21c9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union mask to the nsdgeneral ROI and convert to nifti\n",
    "assert mask_img.get_fdata().sum() == union_mask.shape\n",
    "union_mask_img = new_img_like(mask_img, union_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4768e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union_mask to mask_img and return nifti object\n",
    "\n",
    "# Get the data as a boolean array\n",
    "mask_data = mask_img.get_fdata().astype(bool)\n",
    "\n",
    "# Flatten only the True voxels in the mask\n",
    "true_voxel_indices = np.where(mask_data.ravel())[0]\n",
    "\n",
    "# Apply the union_mask (boolean mask of size 19174)\n",
    "selected_voxel_indices = true_voxel_indices[union_mask]\n",
    "\n",
    "# Create a new flattened mask with all False\n",
    "new_mask_flat = np.zeros(mask_data.size, dtype=bool)\n",
    "\n",
    "# Set selected voxels to True\n",
    "new_mask_flat[selected_voxel_indices] = True\n",
    "\n",
    "# Reshape back to original 3D shape\n",
    "new_mask_data = new_mask_flat.reshape(mask_data.shape)\n",
    "\n",
    "# Create new NIfTI image\n",
    "union_mask_img = nib.Nifti1Image(new_mask_data.astype(np.uint8), affine=mask_img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057b3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union_mask_img.shape (76, 90, 74)\n",
      "union mask num voxels 8627\n"
     ]
    }
   ],
   "source": [
    "print(\"union_mask_img.shape\", union_mask_img.shape)\n",
    "print(\"union mask num voxels\", int(union_mask_img.get_fdata().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895d9228-46e0-4ec0-9fe4-00f802f9708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reconstructions(betas_tt):\n",
    "    \"\"\"\n",
    "    takes in the beta map for a stimulus trial in torch tensor format (tt)\n",
    "\n",
    "    returns reconstructions and clipvoxels for retrievals\n",
    "    \"\"\"\n",
    "    # start_reconstruction_time = time.time()\n",
    "    model.to(device)\n",
    "    model.eval().requires_grad_(False)\n",
    "    clipvoxelsTR = None\n",
    "    reconsTR = None\n",
    "    num_samples_per_image = 1\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        voxel = betas_tt\n",
    "        voxel = voxel.to(device)\n",
    "        voxel_ridge = model.ridge(voxel[:,[0]],0) # 0th index of subj_list\n",
    "        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "        clip_voxels = clip_voxels0\n",
    "        backbone = backbone0\n",
    "        blurry_image_enc = blurry_image_enc0[0]\n",
    "        clipvoxelsTR = clip_voxels.cpu()\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)  \n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils_mindeye.unclip_recon(prior_out[[i]],\n",
    "                            diffusion_engine,\n",
    "                            vector_suffix,\n",
    "                            num_samples=num_samples_per_image)\n",
    "            if reconsTR is None:\n",
    "                reconsTR = samples.cpu()\n",
    "            else:\n",
    "                reconsTR = torch.vstack((reconsTR, samples.cpu()))\n",
    "            imsize = 224\n",
    "            reconsTR = transforms.Resize((imsize,imsize), antialias=True)(reconsTR).float().numpy().tolist()\n",
    "        return reconsTR, clipvoxelsTR\n",
    "    \n",
    "def batchwise_cosine_similarity(Z,B):\n",
    "    Z = Z.flatten(1)\n",
    "    B = B.flatten(1).T\n",
    "    Z_norm = torch.linalg.norm(Z, dim=1, keepdim=True)  # Size (n, 1).\n",
    "    B_norm = torch.linalg.norm(B, dim=0, keepdim=True)  # Size (1, b).\n",
    "    cosine_similarity = ((Z @ B) / (Z_norm @ B_norm)).T\n",
    "    return cosine_similarity\n",
    "\n",
    "def get_top_retrievals(clipvoxel, all_images, total_retrievals = 1):\n",
    "    '''\n",
    "    clipvoxel: output from do_recons that contains that information needed for retrievals\n",
    "    all_images: all ground truth actually seen images by the participant in day 2 run 1\n",
    "\n",
    "    outputs the top retrievals\n",
    "    '''\n",
    "    values_dict = {}\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        emb = clip_img_embedder(torch.reshape(all_images,(all_images.shape[0], 3, 224, 224)).to(device)).float() # CLIP-Image\n",
    "        emb = emb.cpu()\n",
    "        emb_ = clipvoxel # CLIP-Brain\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = np.reshape(emb_, (1, 425984))\n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        emb_ = emb_.float()\n",
    "        fwd_sim = batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "        print(\"Given Brain embedding, find correct Image embedding\")\n",
    "    fwd_sim = np.array(fwd_sim.cpu())\n",
    "    which = np.flip(np.argsort(fwd_sim, axis = 0))\n",
    "    imsize = 224\n",
    "    for attempt in range(total_retrievals):\n",
    "        values_dict[f\"attempt{(attempt+1)}\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[which[attempt].copy()]).float().numpy().tolist()\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "def convert_image_array_to_PIL(image_array):\n",
    "    if image_array.ndim == 4:\n",
    "        image_array = image_array[0]\n",
    "\n",
    "    # get the dimension to h, w, 3|1\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "    \n",
    "    # clip the image array to 0-1\n",
    "    image_array = np.clip(image_array, 0, 1)\n",
    "    # convert the image array to uint8\n",
    "    image_array = (image_array * 255).astype('uint8')\n",
    "    # convert the image array to PIL\n",
    "    return Image.fromarray(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e595bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_glm import load_glmsingle_hrf_library, hrf_i_factory, fit_and_run_glm\n",
    "BASE_TIME, GLMS_HRFS = load_glmsingle_hrf_library(f\"{data_path}/getcanonicalhrflibrary.tsv\")\n",
    "hrf_fns = [hrf_i_factory(i, BASE_TIME, GLMS_HRFS) for i in range(1, 21)]\n",
    "hrf_indices = np.load(f\"{data_path}/avg_hrfs_s1_s2_full.npy\").astype(int)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff1807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images=True\n",
    "save_individual_images=False\n",
    "only_betas = False  # skip plotting and saving, only calculate betas (located in all_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a4d40-643d-493b-874b-2030490b9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 started\n",
      "TR 0\n",
      "blank\n",
      "TR 1\n",
      "blank\n",
      "TR 2\n",
      "blank\n",
      "TR 3\n",
      "blank\n",
      "TR 4\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_7211_cocoid_59250.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 5\n",
      "blank\n",
      "TR 6\n",
      "blank\n",
      "TR 7\n",
      "all_stimuli/special515/special_67295.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 8\n",
      "blank\n",
      "TR 9\n",
      "blank\n",
      "TR 10\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_5729_cocoid_53029.png\n",
      "TR 11\n",
      "blank\n",
      "TR 12\n",
      "all_stimuli/special515/special_70232.jpg\n",
      "TR 13\n",
      "blank\n",
      "TR 14\n",
      "blank\n",
      "TR 15\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_7251_cocoid_26645.png\n",
      "TR 16\n",
      "blank\n",
      "TR 17\n",
      "blank\n",
      "TR 18\n",
      "all_stimuli/special515/special_40721.jpg\n",
      "TR 19\n",
      "blank\n",
      "TR 20\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_1956_cocoid_70856.png\n"
     ]
    }
   ],
   "source": [
    "mc_dir = f\"{derivatives_path}/motion_corrected\"\n",
    "mc_resampled_dir = f\"{derivatives_path}/motion_corrected_resampled\"\n",
    "if os.path.exists(mc_dir):\n",
    "    shutil.rmtree(mc_dir)\n",
    "os.makedirs(mc_dir)\n",
    "if os.path.exists(mc_resampled_dir):\n",
    "    shutil.rmtree(mc_resampled_dir)\n",
    "os.makedirs(mc_resampled_dir)\n",
    "\n",
    "ses3_to_ses1_mat = f'{derivatives_path}/ses3ref_to_ses1ref'\n",
    "# set the output type to NIFTI_GZ\n",
    "os.environ['FSLOUTPUTTYPE'] = 'NIFTI_GZ'\n",
    "assert np.all(ses1_boldref_nib.affine == union_mask_img.affine)\n",
    "all_betas = []\n",
    "\n",
    "# Loop over all 11 runs in the session\n",
    "n_runs = 11\n",
    "for run_num in range(1, n_runs + 1):\n",
    "    print(f\"Run {run_num} started\")\n",
    "    mc_params = []\n",
    "    imgs = []\n",
    "    events_df = ndscore_events[run_num - 1]\n",
    "    tr_labels_hrf = ndscore_tr_labels[run_num - 1][\"tr_label_hrf\"].tolist()\n",
    "    events_df = events_df[events_df['image_name'] != 'blank.jpg']  # must drop blank.jpg after tr_labels_hrf is defined to keep indexing consistent\n",
    "    beta_maps_list = []\n",
    "    all_trial_names_list = []\n",
    "    all_images = None\n",
    "\n",
    "    # define save_path\n",
    "    save_path = f\"{derivatives_path}/{sub}_{session}_task-{func_task_name}_run-{run_num:02d}_recons\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_individual_images = True\n",
    "    if save_individual_images:\n",
    "        os.makedirs(os.path.join(save_path, \"individual_images\"), exist_ok=True)\n",
    "\n",
    "    all_recons_save = []\n",
    "    all_clipvoxels_save = []\n",
    "    all_ground_truth_save = []\n",
    "    all_retrieved_save = []\n",
    "\n",
    "    stimulus_trial_counter = 0\n",
    "    plot_images = True\n",
    "    T1_brain = f\"{data_path}/sub-005_desc-preproc_T1w_brain.nii.gz\"\n",
    "    n_trs = 192\n",
    "    assert len(tr_labels_hrf) == n_trs, \"there should be image labels for each TR\"\n",
    "    assert all(label in image_names for label in tr_labels_hrf if label != 'blank'), \"Some labels in tr_labels_hrf are missing from image_names.\"\n",
    "    assert len(images) > n_trs, \"images array is too short.\"\n",
    "    for TR in range(n_trs-1):\n",
    "        print(f\"TR {TR}\")\n",
    "        # stream in the nifti\n",
    "        cur_vol = f\"{sub}_{session}_task-{func_task_name}_run-{run_num:02d}_bold_{TR:04d}\"\n",
    "        curr_image_path = os.path.join(single_vols_path, f\"{cur_vol}.nii.gz\")\n",
    "        image_data = nib.load(curr_image_path)\n",
    "        current_label = tr_labels_hrf[TR]\n",
    "        print(current_label)\n",
    "        if TR == 0 and run_num == 1:\n",
    "            # ses3_vol0_nib = image_data\n",
    "            # make the day 2 bold ref\n",
    "            nib.save(image_data, ses3_vol0)\n",
    "            # save the transformation from the day 2 bold ref to the day 1 \n",
    "            # os.system(f\"antsRegistrationSyNQuick.sh \\\n",
    "            #   -d 3 \\\n",
    "            #   -f {T1_brain} \\\n",
    "            #   -m {ses3_vol0} \\\n",
    "            #   -o {derivatives_path}/ses3_vol0_epi2T1_\")\n",
    "\n",
    "            os.system(f\"flirt -in {ses3_vol0} \\\n",
    "                -ref {ses1_boldref} \\\n",
    "                -omat {ses3_to_ses1_mat} \\\n",
    "                -dof 6\")\n",
    "\n",
    "            # for simulation, just load it in\n",
    "            # ses3_boldref_path = f\"{derivatives_path}/ses3_vol0_epi2T1_Warped.nii.gz\"\n",
    "            # ses3_boldref = nib.load(ses3_boldref_path)\n",
    "        # load nifti file\n",
    "        # tmp = f'{storage_path}/day2_subj1/tmp_run{run_num}.nii.gz'\n",
    "        # nib.save(index_img(image_data,0),tmp)\n",
    "        start = time.time()\n",
    "        mc = f\"{mc_dir}/{cur_vol}_mc\"\n",
    "        os.system(f\"{fsl_path}/mcflirt -in {os.path.join(single_vols_path, f'{cur_vol}.nii.gz')} -reffile {os.path.join(single_vols_path, f'{sub}_{session}_task-{func_task_name}_run-01_bold_0000.nii.gz')} -out {mc} -plots -mats\")\n",
    "        mc_params.append(np.loadtxt(f'{mc}.par'))\n",
    "\n",
    "        current_tr_to_ses1 = f\"{derivatives_path}/current_tr_to_ses1_run{run_num}\"\n",
    "        os.system(f\"convert_xfm -concat {ses3_to_ses1_mat} -omat {current_tr_to_ses1} {mc}.mat/MAT_0000\")    \n",
    "        \n",
    "        # apply concatenated matrix to the current TR\n",
    "        final_vol = f\"{mc_resampled_dir}/{session}_run-{run_num:02d}_{TR:04d}_mc_boldres.nii.gz\"\n",
    "        os.system(f\"flirt -in {curr_image_path} \\\n",
    "            -ref {ses1_boldref} \\\n",
    "            -out {final_vol} \\\n",
    "            -init {current_tr_to_ses1} \\\n",
    "            -applyxfm\")\n",
    "\n",
    "        os.system(f\"rm -r {mc}.mat\")\n",
    "        imgs.append(get_data(final_vol))\n",
    "        \n",
    "        if current_label not in ('blank', 'blank.jpg'):\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] = events_df['onset'].astype(float)\n",
    "\n",
    "            run_start_time = events_df['onset'].iloc[0]\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] -= run_start_time\n",
    "\n",
    "            cropped_events = events_df[events_df.onset <= TR*tr_length]\n",
    "            cropped_events = cropped_events.copy()\n",
    "            cropped_events.loc[:, 'trial_type'] = np.where(cropped_events['trial_number'] == stimulus_trial_counter, \"probe\", \"reference\")\n",
    "            cropped_events = cropped_events.drop(columns=['is_correct', 'image_name', 'response_time', 'trial_number'])\n",
    "\n",
    "            # collect all of the images at each TR into a 4D time series\n",
    "            img = np.rollaxis(np.array(imgs),0,4)\n",
    "            img = new_img_like(ses1_boldref_nib,img,copy_header=True)\n",
    "            # run the model with mc_params confounds to motion correct\n",
    "            lss_glms = [ FirstLevelModel(t_r=tr_length,slice_time_ref=0,hrf_model=hrf_fn,\n",
    "                        drift_model='cosine', drift_order=1,high_pass=0.01,mask_img=union_mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True) for hrf_fn in hrf_fns]\n",
    "            \n",
    "            beta_map_np, dm = fit_and_run_glm(lss_glms, img, cropped_events, pd.DataFrame(np.array(mc_params)), hrfs_indices=hrf_indices)\n",
    "            # beta_map_np = beta_map.get_fdata()\n",
    "            beta_map_np = fast_apply_mask(target=beta_map_np,mask=union_mask_img.get_fdata())\n",
    "            all_betas.append(beta_map_np)\n",
    "            \n",
    "            if only_betas:\n",
    "                continue\n",
    "            if \"MST_pairs\" in current_label:\n",
    "                correct_image_index = np.where(current_label == vox_image_names)[0][0]  # using the first occurrence based on image name, assumes that repeated images are identical (which they should be)\n",
    "                z_mean = np.mean(np.array(all_betas), axis=0)\n",
    "                z_std = np.std(np.array(all_betas), axis=0)\n",
    "                betas = ((np.array(all_betas) - z_mean) / (z_std + 1e-6))[-1]  # use only the beta pattern from the most recent image\n",
    "                betas = betas[np.newaxis, np.newaxis, :]\n",
    "                betas_tt = torch.Tensor(betas).to(\"cpu\")\n",
    "                reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "                if clipvoxelsTR is None:\n",
    "                    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                        voxel = betas_tt\n",
    "                        voxel = voxel.to(device)\n",
    "                        assert voxel.shape[1] == 1\n",
    "                        voxel_ridge = model.ridge(voxel[:,[-1]],0) # 0th index of subj_list\n",
    "                        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                        blurry_image_enc = blurry_image_enc0[0]\n",
    "                        clipvoxelsTR = clip_voxels.cpu()\n",
    "                values_dict = get_top_retrievals(clipvoxelsTR, all_images=images[MST_idx], total_retrievals=5)\n",
    "                image_array = np.array(reconsTR)[0]\n",
    "                # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                    image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # Display the image\n",
    "                if plot_images:\n",
    "                    # plot original and reconstructed images\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.title(\"Reconstructed Image\")\n",
    "                    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot original with top 5 retrievals\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 6, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    for i in range(5):\n",
    "                        plt.subplot(1, 6, i+2)\n",
    "                        plt.title(f\"Retrieval {i+1}\")\n",
    "                        plt.imshow(np.array(values_dict[f\"attempt{i+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "                        plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                # save reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "                if save_individual_images:\n",
    "                    # save the reconstructed image\n",
    "                    convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_reconstructed.png\"))\n",
    "                    # save the retrieved images\n",
    "                    for key, value in values_dict.items():\n",
    "                        if (not ('ground_truth' in key)):\n",
    "                            convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_retrieved_{key}.png\"))\n",
    "                    # save the clip_voxels\n",
    "                    np.save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "                    # save the ground truth image\n",
    "                    convert_image_array_to_PIL(images[correct_image_index].numpy()).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_ground_truth.png\"))\n",
    "                all_recons_save.append(image_array)\n",
    "                all_clipvoxels_save.append(clipvoxelsTR)\n",
    "                all_ground_truth_save.append(images[correct_image_index].numpy())\n",
    "                all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if (not ('ground_truth' in key))])\n",
    "            else:\n",
    "                pass\n",
    "            stimulus_trial_counter += 1\n",
    "        elif current_label == 'blank.jpg':\n",
    "            stimulus_trial_counter += 1\n",
    "        else:\n",
    "            assert current_label == 'blank'\n",
    "\n",
    "    # save betas so far\n",
    "    np.save(os.path.join(save_path, f\"betas_run-{run_num:02d}.npy\"), np.array(all_betas))\n",
    "    if only_betas:\n",
    "        continue\n",
    "    # save the design matrix for the current run\n",
    "    dm.to_csv(os.path.join(save_path, f\"design_run-{run_num:02d}.csv\"))\n",
    "    plot_design_matrix(dm, output_file=os.path.join(save_path, \"dm\"))\n",
    "    dm[['probe_hrf_callable', 'reference_hrf_callable']].plot(title='Probe/Reference Regressors', figsize=(10, 4))\n",
    "    plt.savefig(os.path.join(save_path, \"regressors\"))\n",
    "    print(f\"==END OF RUN {run_num}!==\\n\")\n",
    "    # save the tensors\n",
    "    if all_recons_save:\n",
    "        all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "        all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "        all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "        all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "        torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "        torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "        print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "        print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "        print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "        print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "        print(\"All tensors saved successfully on \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec54d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display all recons with originals\n",
    "# all_recons_save = torch.load(os.path.join(save_path, \"all_recons.pt\"))\n",
    "# for i in range(len(all_recons_save)):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.title(\"Reconstructed Image\")\n",
    "#     plt.imshow(all_recons_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display top 5 retrievals and originals\n",
    "# for i in range(len(all_retrieved_save)):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.subplot(1, 6, 1)\n",
    "#     plt.title(\"Original\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     for j in range(5):\n",
    "#         plt.subplot(1, 6, j+2)\n",
    "#         plt.title(f\"Top {j+1}\")\n",
    "#         plt.imshow(all_retrieved_save[i][j][0].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#         plt.axis('off')  # Hide axes\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_recons_and_evaluate(\n",
    "#     beta_series,  # shape: (n_conditions, n_voxels)\n",
    "#     images,       # tensor of all images, shape: (n_images, 3, 224, 224)\n",
    "#     model,        # loaded model\n",
    "#     do_reconstructions,  # function for recon\n",
    "#     device,       # torch device\n",
    "#     save_path=None,    # directory to save results\n",
    "#     metrics_module=None,  # module or dict with metric functions, optional\n",
    "#     save_results=False,   # flag to control saving, defaults to False\n",
    "#     test_idx=None,         # indices for test set, optional\n",
    "#     do_zscore=False\n",
    "# ):\n",
    "#     if test_idx is not None:\n",
    "#         beta_series = beta_series[test_idx]\n",
    "#         images = images[test_idx]\n",
    "\n",
    "#     all_recons_save_tensor = []\n",
    "#     all_clipvoxels_save_tensor = []\n",
    "#     all_ground_truth_save_tensor = []\n",
    "\n",
    "#     for idx in range(beta_series.shape[0]):\n",
    "#         beta_pattern = beta_series[np.newaxis, np.newaxis, idx, :]  # (1,1,n_vox)\n",
    "#         if do_zscore:\n",
    "#             # Z-score using only betas up to and including the current image\n",
    "#             beta_series_up_to_now = beta_series[:idx+1]\n",
    "#             beta_pattern = utils_mindeye.zscore(beta_pattern, beta_series_up_to_now)\n",
    "\n",
    "#         betas_tt = torch.Tensor(beta_pattern).to(\"cpu\")\n",
    "#         reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "#         if clipvoxelsTR is None:\n",
    "#             with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "#                 voxel = betas_tt.to(device)\n",
    "#                 assert voxel.shape[1] == 1\n",
    "#                 voxel_ridge = model.ridge(voxel[:, [-1]], 0)\n",
    "#                 backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "#                 clipvoxelsTR = clip_voxels0.cpu()\n",
    "\n",
    "#         image_array = np.array(reconsTR)[0]\n",
    "#         if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "#             image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "#         all_recons_save_tensor.append(reconsTR)\n",
    "#         all_clipvoxels_save_tensor.append(clipvoxelsTR)\n",
    "#         all_ground_truth_save_tensor.append(images[idx])\n",
    "\n",
    "#     all_recons_save_tensor = torch.stack(all_recons_save_tensor)\n",
    "#     all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save_tensor)\n",
    "#     all_ground_truth_save_tensor = torch.stack(all_ground_truth_save_tensor)\n",
    "\n",
    "#     if save_results and save_path is not None:\n",
    "#         os.makedirs(save_path, exist_ok=True)\n",
    "#         torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "#         torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "#         torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "\n",
    "#     if metrics_module is not None:\n",
    "#         with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "#             unique_clip_voxels = all_clipvoxels_save_tensor\n",
    "#             unique_ground_truth = all_ground_truth_save_tensor\n",
    "#             all_fwd_acc, all_bwd_acc = metrics_module['calculate_retrieval_metrics'](unique_clip_voxels, unique_ground_truth)\n",
    "#             pixcorr = metrics_module['calculate_pixcorr'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             ssim_ = metrics_module['calculate_ssim'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             alexnet2, alexnet5 = metrics_module['calculate_alexnet'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             inception = metrics_module['calculate_inception_v3'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             clip_ = metrics_module['calculate_clip'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             efficientnet = metrics_module['calculate_efficientnet_b1'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             swav = metrics_module['calculate_swav'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "#         df_metrics = pd.DataFrame({\n",
    "#             \"Metric\": [\n",
    "#                 \"alexnet2\",\n",
    "#                 \"alexnet5\",\n",
    "#                 \"inception\",\n",
    "#                 \"clip_\",\n",
    "#                 \"efficientnet\",\n",
    "#                 \"swav\",\n",
    "#                 \"pixcorr\",\n",
    "#                 \"ssim\",\n",
    "#                 \"all_fwd_acc\",\n",
    "#                 \"all_bwd_acc\"\n",
    "#             ],\n",
    "#             \"Value\": [\n",
    "#                 alexnet2,\n",
    "#                 alexnet5,\n",
    "#                 inception,\n",
    "#                 clip_,\n",
    "#                 efficientnet,\n",
    "#                 swav,\n",
    "#                 pixcorr,\n",
    "#                 ssim_,\n",
    "#                 all_fwd_acc,\n",
    "#                 all_bwd_acc\n",
    "#             ]\n",
    "#         })\n",
    "#         df_metrics.set_index(\"Metric\", inplace=True)\n",
    "#         if save_results and save_path is not None:\n",
    "#             df_metrics.to_csv(os.path.join(save_path, \"metrics.csv\"))\n",
    "#         print(df_metrics)\n",
    "\n",
    "#     return all_recons_save_tensor, all_clipvoxels_save_tensor, all_ground_truth_save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6675825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating retrieval subset 0 (first set of repeats)\n",
      "Loading clip_img_embedder\n",
      "The total pool of images and clip voxels to do retrieval on is:  62\n",
      "Creating embeddings for images\n",
      "Calculating retrieval metrics\n",
      "overall fwd percent_correct: 0.2742\n",
      "overall bwd percent_correct: 0.2581\n",
      "calculating retrieval subset 1 (second set of repeats)\n",
      "Loading clip_img_embedder\n",
      "The total pool of images and clip voxels to do retrieval on is:  62\n",
      "Creating embeddings for images\n",
      "Calculating retrieval metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall fwd percent_correct: 0.2742\n",
      "overall bwd percent_correct: 0.3226\n",
      "torch.Size([124, 541875])\n",
      "torch.Size([124, 541875])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 247.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Correlation: 0.04724747075372534\n",
      "converted, now calculating ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:01<00:00, 88.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.32484267445458714\n",
      "Loading AlexNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---early, AlexNet(2)---\n",
      "2-way Percent Correct (early AlexNet): 0.6292\n",
      "\n",
      "---mid, AlexNet(5)---\n",
      "2-way Percent Correct (mid AlexNet): 0.6320\n",
      "Loading Inception V3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-way Percent Correct (Inception V3): 0.5408\n",
      "Loading CLIP\n",
      "2-way Percent Correct (CLIP): 0.5080\n",
      "Loading EfficientNet B1\n",
      "Distance EfficientNet B1: 0.9474694025082397\n",
      "Loading SwAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ri4541@pu.win.princeton.edu/.cache/torch/hub/facebookresearch_swav_main\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ri4541@pu.win.princeton.edu/rt-cloud/rtcloud/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance SwAV: 0.6157801625942667\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation metrics\n",
    "from utils_mindeye import calculate_retrieval_metrics, calculate_alexnet, calculate_clip, calculate_swav, calculate_efficientnet_b1, calculate_inception_v3, calculate_pixcorr, calculate_ssim, deduplicate_tensors\n",
    "all_recons_save_tensor = []\n",
    "all_clipvoxels_save_tensor = []\n",
    "all_ground_truth_save_tensor = []\n",
    "all_retrieved_save_tensor = []\n",
    "\n",
    "for run_num in range(n_runs):\n",
    "    save_path = f\"{derivatives_path}/{sub}_{session}_task-{func_task_name}_run-{run_num+1:02d}_recons\"\n",
    "\n",
    "    try:\n",
    "        # recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16)\n",
    "        # clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16)\n",
    "        # ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16)\n",
    "        recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16).to(device)\n",
    "        clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16).to(device)\n",
    "        ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16).to(device)\n",
    "\n",
    "        all_recons_save_tensor.append(recons)\n",
    "        all_clipvoxels_save_tensor.append(clipvoxels)\n",
    "        all_ground_truth_save_tensor.append(ground_truth)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Tensors not found. Please check the save path.\")\n",
    "\n",
    "# Concatenate tensors along the first dimension\n",
    "try:\n",
    "    all_recons_save_tensor = torch.cat(all_recons_save_tensor, dim=0)\n",
    "    all_clipvoxels_save_tensor = torch.cat(all_clipvoxels_save_tensor, dim=0)\n",
    "    all_ground_truth_save_tensor = torch.cat(all_ground_truth_save_tensor, dim=0)\n",
    "except RuntimeError:\n",
    "    print('Error: Couldn\\'t concatenate tensors')\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    unique_clip_voxels, unique_ground_truth, duplicated = deduplicate_tensors(all_clipvoxels_save_tensor, all_ground_truth_save_tensor)\n",
    "    \n",
    "    print('calculating retrieval subset 0 (first set of repeats)')\n",
    "    unique_clip_voxels_subset0 = all_clipvoxels_save_tensor[np.array(duplicated)[:,0]]\n",
    "    unique_ground_truth_subset0 = all_ground_truth_save_tensor[np.array(duplicated)[:,0]]\n",
    "    all_fwd_acc_subset0, all_bwd_acc_subset0 = calculate_retrieval_metrics(unique_clip_voxels_subset0, unique_ground_truth_subset0)\n",
    "\n",
    "    print('calculating retrieval subset 1 (second set of repeats)')\n",
    "    unique_clip_voxels_subset1 = all_clipvoxels_save_tensor[np.array(duplicated)[:,1]]\n",
    "    unique_ground_truth_subset1 = all_ground_truth_save_tensor[np.array(duplicated)[:,1]]\n",
    "    all_fwd_acc_subset1, all_bwd_acc_subset1 = calculate_retrieval_metrics(unique_clip_voxels_subset1, unique_ground_truth_subset1)\n",
    "    pixcorr = calculate_pixcorr(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    ssim_ = calculate_ssim(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    alexnet2, alexnet5 = calculate_alexnet(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    inception = calculate_inception_v3(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    clip_ = calculate_clip(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    efficientnet = calculate_efficientnet_b1(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    swav = calculate_swav(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "\n",
    "# save the results to a csv file\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"alexnet2\",\n",
    "        \"alexnet5\",\n",
    "        \"inception\",\n",
    "        \"clip_\",\n",
    "        \"efficientnet\",\n",
    "        \"swav\",\n",
    "        \"pixcorr\",\n",
    "        \"ssim\",\n",
    "        \"all_fwd_acc_subset0\",\n",
    "        \"all_bwd_acc_subset0\",\n",
    "        \"all_fwd_acc_subset1\",\n",
    "        \"all_bwd_acc_subset1\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        alexnet2,\n",
    "        alexnet5,\n",
    "        inception,\n",
    "        clip_,\n",
    "        efficientnet,\n",
    "        swav,\n",
    "        pixcorr,\n",
    "        ssim_,\n",
    "        all_fwd_acc_subset0,\n",
    "        all_bwd_acc_subset0,\n",
    "        all_fwd_acc_subset1,\n",
    "        all_bwd_acc_subset1\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52e71597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formatted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexnet2</th>\n",
       "      <td>62.92% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexnet5</th>\n",
       "      <td>63.20% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inception</th>\n",
       "      <td>54.08% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clip_</th>\n",
       "      <td>50.80% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnet</th>\n",
       "      <td>0.95 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swav</th>\n",
       "      <td>0.62 ↓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixcorr</th>\n",
       "      <td>0.05 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssim</th>\n",
       "      <td>0.32 ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_fwd_acc_subset0</th>\n",
       "      <td>27.42% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_bwd_acc_subset0</th>\n",
       "      <td>25.81% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_fwd_acc_subset1</th>\n",
       "      <td>27.42% ↑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_bwd_acc_subset1</th>\n",
       "      <td>32.26% ↑</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Formatted\n",
       "Metric                       \n",
       "alexnet2             62.92% ↑\n",
       "alexnet5             63.20% ↑\n",
       "inception            54.08% ↑\n",
       "clip_                50.80% ↑\n",
       "efficientnet           0.95 ↓\n",
       "swav                   0.62 ↓\n",
       "pixcorr                0.05 ↑\n",
       "ssim                   0.32 ↑\n",
       "all_fwd_acc_subset0  27.42% ↑\n",
       "all_bwd_acc_subset0  25.81% ↑\n",
       "all_fwd_acc_subset1  27.42% ↑\n",
       "all_bwd_acc_subset1  32.26% ↑"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "percentage_metrics = [\"alexnet2\", \"alexnet5\", \"inception\", \"clip_\", \"all_fwd_acc_subset0\", \"all_bwd_acc_subset0\", \"all_bwd_acc_subset1\", \"all_fwd_acc_subset1\"]\n",
    "lower_better_metrics = [\"efficientnet\", \"swav\"]\n",
    "higher_better_arrow = \"↑\"\n",
    "lower_better_arrow = \"↓\"\n",
    "\n",
    "# Format function\n",
    "def format_metric(metric, value):\n",
    "    if metric in percentage_metrics:\n",
    "        return f\"{value * 100:.2f}% {higher_better_arrow}\"\n",
    "    elif metric in lower_better_metrics:\n",
    "        return f\"{value:.2f} {lower_better_arrow}\"\n",
    "    else:\n",
    "        return f\"{value:.2f} {higher_better_arrow}\"\n",
    "\n",
    "# Apply formatting\n",
    "df_formatted = df_metrics.copy()\n",
    "df_formatted[\"Formatted\"] = df_formatted.apply(lambda row: format_metric(row[\"Metric\"], row[\"Value\"]), axis=1)\n",
    "df_formatted.set_index(\"Metric\", inplace=True)\n",
    "df_formatted.index.name = \"Metric\"\n",
    "\n",
    "# Print and save\n",
    "display(df_formatted[[\"Formatted\"]])\n",
    "\n",
    "df_formatted[[\"Formatted\"]].to_csv(os.path.join(save_path, \"metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = df_metrics.copy()\n",
    "df_formatted[\"Formatted\"] = df_formatted.apply(lambda row: format_metric(row[\"Metric\"], row[\"Value\"]), axis=1)\n",
    "\n",
    "# Print and save\n",
    "# display(df_formatted[[\"Formatted\"]])\n",
    "display(df_formatted[['Metric', 'Formatted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238af6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clip_voxels_subset0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mst_pairs(mst_pairs, unique_clip_voxels_subset0, unique_clip_voxels_subset1, unique_ground_truth_subset0, unique_ground_truth_subset1):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type):\n",
    "        failed_A = []\n",
    "        failed_B = []\n",
    "        failed_non_corr = []\n",
    "\n",
    "        # Get all unique image indices\n",
    "        # all_indices = np.unique(mst_pairs.flatten())\n",
    "        \n",
    "        # # Pre-load all images and betas to device\n",
    "        # all_images = images[image_idx[all_indices]].to(device)\n",
    "        # all_voxels = torch.Tensor(vox[image_idx[all_indices]]).unsqueeze(1).to(device)\n",
    "        \n",
    "        # # Get CLIP embeddings for all images\n",
    "        # all_clip_targets = clip_img_embedder(all_images.float())\n",
    "        # all_clip_targets_norm = nn.functional.normalize(all_clip_targets.flatten(1), dim=-1)\n",
    "        \n",
    "        # # Pass all betas through model to get MindEye embeddings\n",
    "        # all_voxel_ridge = model.ridge(all_voxels, 0)\n",
    "        # _, all_clip_voxels, _ = model.backbone(all_voxel_ridge)\n",
    "        \n",
    "        # unique_clip_voxels_subset0\n",
    "        # all_clip_voxels_norm = nn.functional.normalize(all_clip_voxels.flatten(1), dim=-1)\n",
    "        \n",
    "        # # Dict mapping idx (which indexes the \"vox\" and \"images\" tensors) to pos (their position in the flattened array \"all_indices\")\n",
    "        # idx_to_pos = {idx: pos for pos, idx in enumerate(all_indices)}\n",
    "        \n",
    "        # Initialize scores\n",
    "        corr_score = 0\n",
    "        non_corr_score = 0\n",
    "        corr_total = len(mst_pairs) * 2\n",
    "        non_corr_total = len(mst_pairs) * (len(mst_pairs)-1) * 4  # number of elements in the matrix excluding the diagonal is n*(n-1)*4 since we're doing this twice each for pairmate A and B\n",
    "\n",
    "        # Prepare the data\n",
    "        voxA_embeddings = unique_clip_voxels_subset0\n",
    "        voxB_embeddings = unique_clip_voxels_subset1\n",
    "        imgA_embeddings = unique_ground_truth_subset0\n",
    "        imgB_embeddings = unique_ground_truth_subset1\n",
    "\n",
    "        # Calculate similarities\n",
    "        simA_A = utils_mindeye.batchwise_cosine_similarity(voxA_embeddings, imgA_embeddings)\n",
    "        simA_B = utils_mindeye.batchwise_cosine_similarity(voxA_embeddings, imgB_embeddings)\n",
    "        simB_B = utils_mindeye.batchwise_cosine_similarity(voxB_embeddings, imgB_embeddings)\n",
    "        simB_A = utils_mindeye.batchwise_cosine_similarity(voxB_embeddings, imgA_embeddings)\n",
    "\n",
    "        # Corresponding 2-AFC\n",
    "        correct_A = torch.diag(simA_A) > torch.diag(simA_B)\n",
    "        correct_B = torch.diag(simB_B) > torch.diag(simB_A)\n",
    "\n",
    "        corr_score = correct_A.sum().item() + correct_B.sum().item()\n",
    "\n",
    "        # Store indices where AFC fails\n",
    "        failed_A = [i for i, correct in enumerate(correct_A.cpu()) if not correct]\n",
    "        failed_B = [i for i, correct in enumerate(correct_B.cpu()) if not correct]\n",
    "\n",
    "        # Non-corresponding 2-AFC\n",
    "        N = len(voxA_embeddings)  # Assuming both subsets are of the same length\n",
    "        row_idx = torch.arange(N).unsqueeze(1)\n",
    "        col_idx = torch.arange(N).unsqueeze(0)\n",
    "        off_diag_mask = row_idx != col_idx\n",
    "\n",
    "        diagA_A = simA_A.diag().unsqueeze(1).expand(-1, N)\n",
    "        diagB_B = simB_B.diag().unsqueeze(1).expand(-1, N)\n",
    "\n",
    "        off_diag_mask_device = off_diag_mask.to(simA_A.device)\n",
    "\n",
    "        fail_AA = (simA_A < diagA_A) & off_diag_mask_device\n",
    "        fail_AB = (simA_B < diagA_A) & off_diag_mask_device\n",
    "        fail_BB = (simB_B < diagB_B) & off_diag_mask_device\n",
    "        fail_BA = (simB_A < diagB_B) & off_diag_mask_device\n",
    "\n",
    "        non_corr_score = fail_AA.sum().item() + fail_AB.sum().item() + fail_BB.sum().item() + fail_BA.sum().item()\n",
    "\n",
    "        # Log failed indices\n",
    "        fail_sources = [fail_AA, fail_AB, fail_BB, fail_BA]\n",
    "        failed_non_corr = []\n",
    "        for fail_matrix, label in zip(fail_sources, [\"AA\", \"AB\", \"BB\", \"BA\"]):\n",
    "            fail_coords = torch.nonzero(fail_matrix, as_tuple=False).cpu().numpy()\n",
    "            for i, j in fail_coords:\n",
    "                failed_non_corr.append({\"type\": label, \"i\": i, \"j\": j})\n",
    "\n",
    "    return corr_score, corr_total, int(non_corr_score), non_corr_total, failed_A, failed_B, failed_non_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mst_pairs(MST_paired_idx, unique_clip_voxels_subset0, unique_clip_voxels_subset1, unique_ground_truth_subset0, unique_ground_truth_subset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eece31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = utils_mindeye.find_paired_indices(image_idx)\n",
    "pairs = sorted(pairs, key=lambda x: x[0])\n",
    "\n",
    "evaluate_mst_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfa331",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_MST_images = np.unique(list(all_MST_images.values())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_MST_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56365231",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_pairs = utils_mindeye.find_paired_indices(torch.tensor(MST_ID))\n",
    "MST_pairs = np.array(sorted(MST_pairs[:-1], key=lambda x: x[0]))  # we added a fake value as a placeholder so index out the last group of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3983638",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_paired_idx = []\n",
    "for pair in MST_pairs:\n",
    "    if len(pair) == 2:\n",
    "        assert isinstance(pair, list)\n",
    "        if pair[0] in MST_idx or pair[1] in MST_idx:\n",
    "            MST_paired_idx.append(pair)\n",
    "\n",
    "MST_paired_idx = np.array(MST_paired_idx)\n",
    "print(MST_paired_idx.shape)\n",
    "assert MST_paired_idx.shape == (62,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_MST_images[np.where(['MST_pairs' in x for x in unique_MST_images])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca511725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(list(all_MST_images.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "MST_pairmate_names = unique_MST_images.reshape(int(unique_MST_images.shape[0]/2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [im for im in image_names if str(im) not in ('blank.jpg', 'nan')]\n",
    "assert len(image_idx) == len(x)\n",
    "pairs = []\n",
    "for i, p in enumerate(MST_pairmate_names):\n",
    "    assert p[0] != p[1]  # no duplicate images\n",
    "    pairs.append([utils.find_all_indices(x,p[0]), utils.find_all_indices(x,p[1])])\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a530d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    for j in range(2):\n",
    "        mst_pairs = np.stack([pairs[:, 0, i], pairs[:, 1, j]], axis=1)  # shape (31, 2)\n",
    "        corr_score, corr_total, non_corr_score, non_corr_total, failed_A, failed_B, failed_non_corr = evaluate_mst_pairs(mst_pairs)\n",
    "\n",
    "        # Store scores and failure info together\n",
    "        all_scores.append((corr_score, corr_total, non_corr_score, non_corr_total))\n",
    "        all_failures.append({\n",
    "            \"repeat_A\": i,\n",
    "            \"repeat_B\": j,\n",
    "            \"failed_A\": failed_A,\n",
    "            \"failed_B\": failed_B,\n",
    "            \"failed_non_corr\": failed_non_corr,\n",
    "            \"mst_pairs\": mst_pairs,\n",
    "        })\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"pairmate A repeat {i} vs pairmate B repeat {j}:\")\n",
    "        print(f\"2-AFC corresponding = {corr_score}/{corr_total} ({corr_score/corr_total:.2%})\")\n",
    "        print(f\"2-AFC non-corresponding = {non_corr_score}/{non_corr_total} ({non_corr_score/non_corr_total:.2%})\")\n",
    "        print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtcloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
