{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6053a83-2259-475e-9e21-201e44217e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/conf/.venv/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/scripts\n",
      "line 6:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/scripts\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/scripts\n",
      "line 14:  /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/scripts\n"
     ]
    }
   ],
   "source": [
    "# set up main path where everything will be you should download the\n",
    "# hugging face directory described in readme and put it here on the same\n",
    "# server where the data analyzer is run so that the data analyzer code with \n",
    "# the GPU can access these files\n",
    "# You should replace the below path with your location\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    with open('../conf/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    storage_path = config['storage_path']\n",
    "    data_path = config['data_path']\n",
    "    derivatives_path = config['derivatives_path']\n",
    "    fsl_path = config['fsl_path']\n",
    "    project_path = config['project_path']\n",
    "    assert os.path.exists(storage_path), \"The specified storage path does not exist.\"\n",
    "    assert os.path.exists(data_path), \"The specified data path does not exist.\"\n",
    "    assert os.path.exists(derivatives_path), \"The specified derivatives path does not exist.\"\n",
    "    assert os.path.exists(fsl_path), \"The specified FSL path does not exist.\"\n",
    "    assert os.path.exists(project_path), \"The specified project path does not exist.\"\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"config.json file not found. Please create it with the required paths.\")\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------------------\n",
    "Imports and set up for mindEye\n",
    "-----------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append(f'{project_path}/models/generative_models')\n",
    "sys.path.append(f'{project_path}/models')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# custom functions #\n",
    "import utils_mindeye\n",
    "from models import *\n",
    "import pandas as pd\n",
    "import ants\n",
    "import nilearn\n",
    "import pdb\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4516e788-85cc-42ab-b05a-11bd7207f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir= f\"{storage_path}/cache\"\n",
    "model_name=\"sub-005_ses-01-03_task-C_bs24_MST_rishab_MSTsplit_unionmask_ses-01-03_finetune\"\n",
    "subj=1\n",
    "hidden_dim=1024\n",
    "blurry_recon = False\n",
    "n_blocks=4 \n",
    "seq_len = 1\n",
    "\n",
    "import pickle\n",
    "with open(f\"{storage_path}/clip_img_embedder\", \"rb\") as input_file:\n",
    "    clip_img_embedder = pickle.load(input_file)\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12be1838-f387-4cdd-b7cb-217a74501359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "8,835,072 total\n",
      "8,835,072 trainable\n",
      "param counts:\n",
      "453,360,280 total\n",
      "453,360,280 trainable\n",
      "param counts:\n",
      "462,195,352 total\n",
      "462,195,352 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "722,060,568 total\n",
      "722,060,552 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "722060552"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "num_voxels = 8627\n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "\n",
    "\n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "\n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "\n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                        clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils_mindeye.count_params(model.ridge)\n",
    "utils_mindeye.count_params(model.backbone)\n",
    "utils_mindeye.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils_mindeye.count_params(model.diffusion_prior)\n",
    "utils_mindeye.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a627d35-3cd5-4cd1-9bb3-c02c0c97f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "# Replace with pre_trained_fine_tuned_model.pth\n",
    "# tag='pretrained_fine-tuned_sliceTimed0.5.pth'\n",
    "# tag='pretrained_fine-tuned_sliceTimed.pth'\n",
    "outdir = f'{data_path}/model'\n",
    "\n",
    "# print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "# try:\n",
    "checkpoint = torch.load(outdir+f'/{model_name}.pth', map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "del checkpoint\n",
    "# except: # probably ckpt is saved using deepspeed format\n",
    "#     import deepspeed\n",
    "#     state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "#     model.load_state_dict(state_dict, strict=False)\n",
    "#     del state_dict\n",
    "# print(\"ckpt loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bd11f3-6d4d-4ee4-a23b-443afeb5c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(f\"{project_path}/models/generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "# first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "# first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "with open(f\"{storage_path}/diffusion_engine\", \"rb\") as input_file:\n",
    "    diffusion_engine = pickle.load(input_file)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "    \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "    \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "# f = h5py.File(f'{storage_path}/coco_images_224_float16.hdf5', 'r')\n",
    "# images = f['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cb6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = \"sub-005\"\n",
    "session = \"ses-06\"\n",
    "task = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "func_task_name = 'C'  # 'study' or 'A'; used to search for functional run in bids format\n",
    "n_runs = 11\n",
    "\n",
    "ses_list = [session]\n",
    "design_ses_list = [session]\n",
    "    \n",
    "task_name = f\"_task-{task}\" if task != 'study' else ''\n",
    "designdir = f\"{data_path}/events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48586a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 109)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-06.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 126\n",
      "n_runs 11\n",
      "['all_stimuli/unchosen_nsd_1000_images/unchosen_2458_cocoid_39454.png'\n",
      " 'all_stimuli/unchosen_nsd_1000_images/unchosen_2682_cocoid_72811.png'\n",
      " 'all_stimuli/unchosen_nsd_1000_images/unchosen_6050_cocoid_54489.png'\n",
      " 'all_stimuli/unchosen_nsd_1000_images/unchosen_2549_cocoid_39699.png']\n",
      "[50.54890592 54.55265687 58.56489904 62.5879235 ]\n",
      "[0. 0. 0. 0.]\n",
      "(693,)\n"
     ]
    }
   ],
   "source": [
    "data, starts, images, is_new_run, image_names, unique_images, len_unique_images = utils_mindeye.load_design_files(\n",
    "    sub=sub,\n",
    "    session=session,\n",
    "    func_task_name=task,\n",
    "    designdir=designdir,\n",
    "    design_ses_list=design_ses_list\n",
    ")\n",
    "\n",
    "if sub == 'sub-001':\n",
    "    if session == 'ses-01':\n",
    "        assert image_names[0] == 'images/image_686_seed_1.png'\n",
    "    elif session in ('ses-02', 'all'):\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_40840.jpg'\n",
    "    elif session == 'ses-03':\n",
    "        assert image_names[0] == 'all_stimuli/special515/special_69839.jpg'\n",
    "    elif session == 'ses-04':\n",
    "        assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "elif sub == 'sub-003':\n",
    "    assert image_names[0] == 'all_stimuli/rtmindeye_stimuli/image_686_seed_1.png'\n",
    "\n",
    "unique_images = np.unique(image_names.astype(str))\n",
    "unique_images = unique_images[(unique_images!=\"nan\")]\n",
    "len_unique_images = len(unique_images)\n",
    "\n",
    "print(\"n_runs\",n_runs)\n",
    "\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(unique_images) == 851\n",
    "\n",
    "print(image_names[:4])\n",
    "print(starts[:4])\n",
    "print(is_new_run[:4])\n",
    "\n",
    "image_idx = np.array([])  # contains the unique index of each presented image\n",
    "vox_image_names = np.array([])  # contains the names of the images corresponding to image_idx\n",
    "all_MST_images = dict()\n",
    "for i, im in enumerate(image_names):\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    vox_image_names = np.append(vox_image_names, im)\n",
    "            \n",
    "    image_idx_ = np.where(im==unique_images)[0].item()\n",
    "    image_idx = np.append(image_idx, image_idx_)\n",
    "    \n",
    "    all_MST_images[i] = im\n",
    "    i+=1\n",
    "    \n",
    "image_idx = torch.Tensor(image_idx).long()\n",
    "# for im in new_image_names[MST_images]:\n",
    "#     assert 'MST_pairs' in im\n",
    "# assert len(all_MST_images) == 300\n",
    "\n",
    "unique_MST_images = np.unique(list(all_MST_images.values())) \n",
    "\n",
    "MST_ID = np.array([], dtype=int)\n",
    "\n",
    "vox_idx = np.array([], dtype=int)\n",
    "j=0  # this is a counter keeping track of the remove_random_n used later to index vox based on the removed images; unused otherwise\n",
    "for i, im in enumerate(image_names):  # need unique_MST_images to be defined, so repeating the same loop structure\n",
    "    # skip if blank, nan\n",
    "    if im == \"blank.jpg\":\n",
    "        i+=1\n",
    "        continue\n",
    "    if str(im) == \"nan\":\n",
    "        i+=1\n",
    "        continue\n",
    "    j+=1\n",
    "    curr = np.where(im == unique_MST_images)\n",
    "    # print(curr)\n",
    "    if curr[0].size == 0:\n",
    "        MST_ID = np.append(MST_ID, np.array(len(unique_MST_images)))  # add a value that should be out of range based on the for loop, will index it out later\n",
    "    else:\n",
    "        MST_ID = np.append(MST_ID, curr)\n",
    "        \n",
    "assert len(MST_ID) == len(image_idx)\n",
    "# assert len(np.argwhere(pd.isna(data['current_image']))) + len(np.argwhere(data['current_image'] == 'blank.jpg')) + len(image_idx) == len(data)\n",
    "# MST_ID = torch.tensor(MST_ID[MST_ID != len(unique_MST_images)], dtype=torch.uint8)  # torch.tensor (lowercase) allows dtype kwarg, Tensor (uppercase) is an alias for torch.FloatTensor\n",
    "print(MST_ID.shape)\n",
    "if (sub == 'sub-001' and session == 'ses-04') or (sub == 'sub-003' and session == 'ses-01'):\n",
    "    assert len(all_MST_images) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d16a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 42/693 [00:00<00:01, 412.55it/s]/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/conf/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 693/693 [00:06<00:00, 104.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images torch.Size([693, 3, 224, 224])\n",
      "len MST_images 693\n",
      "MST_images==True 630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "MST_images = []\n",
    "images = None\n",
    "for im_name in tqdm(image_idx):\n",
    "    image_file = f\"{unique_images[im_name]}\"\n",
    "    im = imageio.imread(f\"{data_path}/{image_file}\")\n",
    "    im = torch.Tensor(im / 255).permute(2,0,1)\n",
    "    im = resize_transform(im.unsqueeze(0))\n",
    "    if images is None:\n",
    "        images = im\n",
    "    else:\n",
    "        images = torch.vstack((images, im))\n",
    "    if (\"MST_pairs\" in image_file): # (\"_seed_\" not in unique_images[im_name]) and (unique_images[im_name] != \"blank.jpg\") \n",
    "        MST_images.append(True)\n",
    "    else:\n",
    "        MST_images.append(False)\n",
    "\n",
    "print(\"images\", images.shape)\n",
    "MST_images = np.array(MST_images)\n",
    "print(\"len MST_images\", len(MST_images))\n",
    "if not (sub == 'sub-005' and session == 'ses-06'):\n",
    "    assert len(MST_images[MST_images==True]) == 124\n",
    "print(\"MST_images==True\", len(MST_images[MST_images==True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f692b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs(sub, session, func_task_name, designdir):\n",
    "    \"\"\"Loads design files and processes image pairs for a given session.\"\"\"\n",
    "    _, _, _, _, image_names, unique_images, _ = utils_mindeye.load_design_files(\n",
    "        sub=sub,\n",
    "        session=session,\n",
    "        func_task_name=func_task_name,\n",
    "        designdir=designdir,\n",
    "        design_ses_list=[session]  # Ensure it's a list\n",
    "    )\n",
    "    return utils_mindeye.process_images(image_names, unique_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbffed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (780, 109)\n",
      "Using design file: /home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/3t/data/events/csv/sub-005_ses-06.csv\n",
      "Total number of images: 770\n",
      "Number of unique images: 126\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_dicts = []\n",
    "for s_idx, s in enumerate(ses_list):\n",
    "    im, vo, _ = get_image_pairs(sub, s, func_task_name, designdir)\n",
    "    assert len(im) == len(vo)\n",
    "    all_dicts.append({k:v for k,v in enumerate(vo)})\n",
    "\n",
    "image_to_indices = defaultdict(lambda: [[] for _ in range(len(ses_list))])\n",
    "for ses_idx, idx_to_name in enumerate(all_dicts):\n",
    "    for idx, name in idx_to_name.items():\n",
    "        image_to_indices[name][ses_idx].append(idx)\n",
    "        \n",
    "image_to_indices = dict(image_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b2474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MST_idx 62\n"
     ]
    }
   ],
   "source": [
    "utils_mindeye.seed_everything(0)\n",
    "MST_idx = [v[0][0] if len(v[0]) > 0 else None for k, v in image_to_indices.items() if 'MST_pairs' in k]\n",
    "# Remove any None values (in case some images don't have repeats)\n",
    "MST_idx = [idx for idx in MST_idx if idx is not None]\n",
    "\n",
    "print(\"MST_idx\", len(MST_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e05736bc-c816-49ae-8718-b6c31b412781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.image import get_data, index_img, concat_imgs, new_img_like\n",
    "\n",
    "# get the mask and the reference files\n",
    "ndscore_events = [pd.read_csv(f'{data_path}/events/{sub}_{session}_task-{func_task_name}_run-{run+1:02d}_events.tsv', sep = \"\\t\", header = 0) for run in range(n_runs)]  # create a new list of events_df's which will have the trial_type modified to be unique identifiers\n",
    "ndscore_tr_labels = [pd.read_csv(f\"{data_path}/events/{sub}_{session}_task-{func_task_name}_run-{run+1:02d}_tr_labels.csv\") for run in range(n_runs)]\n",
    "tr_length = 1.5\n",
    "mask_img = nib.load(f'{data_path}/{sub}_final_mask.nii.gz')  # nsdgeneral mask in functional space\n",
    "assert sub == 'sub-005'\n",
    "fmriprep_boldref = f\"{data_path}/sub-005_ses-01_task-C_run-01_space-T1w_boldref.nii.gz\"  # preprocessed boldref from ses-01\n",
    "single_vols_path = f\"{derivatives_path}/vols/{sub}/{session}\"\n",
    "os.makedirs(single_vols_path, exist_ok=True)\n",
    "rt_vol0 = os.path.join(single_vols_path, f\"{sub}_{session}_task-{func_task_name}_run-01_bold_0000.nii.gz\") # first volume (vol0000) of real-time session\n",
    "\n",
    "def fast_apply_mask(target=None,mask=None):\n",
    "    return target[np.where(mask == 1)].T\n",
    "fmriprep_boldref_nib = nib.load(fmriprep_boldref)\n",
    "union_mask = np.load(f\"{data_path}/union_mask_from_ses-01-02.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21c9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union mask to the nsdgeneral ROI and convert to nifti\n",
    "assert mask_img.get_fdata().sum() == union_mask.shape\n",
    "union_mask_img = new_img_like(mask_img, union_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4768e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply union_mask to mask_img and return nifti object\n",
    "\n",
    "# Get the data as a boolean array\n",
    "mask_data = mask_img.get_fdata().astype(bool)\n",
    "\n",
    "# Flatten only the True voxels in the mask\n",
    "true_voxel_indices = np.where(mask_data.ravel())[0]\n",
    "\n",
    "# Apply the union_mask (boolean mask of size 19174)\n",
    "selected_voxel_indices = true_voxel_indices[union_mask]\n",
    "\n",
    "# Create a new flattened mask with all False\n",
    "new_mask_flat = np.zeros(mask_data.size, dtype=bool)\n",
    "\n",
    "# Set selected voxels to True\n",
    "new_mask_flat[selected_voxel_indices] = True\n",
    "\n",
    "# Reshape back to original 3D shape\n",
    "new_mask_data = new_mask_flat.reshape(mask_data.shape)\n",
    "\n",
    "# Create new NIfTI image\n",
    "union_mask_img = nib.Nifti1Image(new_mask_data.astype(np.uint8), affine=mask_img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057b3dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union_mask_img.shape (76, 90, 74)\n",
      "union mask num voxels 8627\n"
     ]
    }
   ],
   "source": [
    "print(\"union_mask_img.shape\", union_mask_img.shape)\n",
    "print(\"union mask num voxels\", int(union_mask_img.get_fdata().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895d9228-46e0-4ec0-9fe4-00f802f9708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reconstructions(betas_tt):\n",
    "    \"\"\"\n",
    "    takes in the beta map for a stimulus trial in torch tensor format (tt)\n",
    "\n",
    "    returns reconstructions and clipvoxels for retrievals\n",
    "    \"\"\"\n",
    "    # start_reconstruction_time = time.time()\n",
    "    model.to(device)\n",
    "    model.eval().requires_grad_(False)\n",
    "    clipvoxelsTR = None\n",
    "    reconsTR = None\n",
    "    num_samples_per_image = 1\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        voxel = betas_tt\n",
    "        voxel = voxel.to(device)\n",
    "        voxel_ridge = model.ridge(voxel[:,[0]],0) # 0th index of subj_list\n",
    "        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "        clip_voxels = clip_voxels0\n",
    "        backbone = backbone0\n",
    "        blurry_image_enc = blurry_image_enc0[0]\n",
    "        clipvoxelsTR = clip_voxels.cpu()\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)  \n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils_mindeye.unclip_recon(prior_out[[i]],\n",
    "                            diffusion_engine,\n",
    "                            vector_suffix,\n",
    "                            num_samples=num_samples_per_image)\n",
    "            if reconsTR is None:\n",
    "                reconsTR = samples.cpu()\n",
    "            else:\n",
    "                reconsTR = torch.vstack((reconsTR, samples.cpu()))\n",
    "            imsize = 224\n",
    "            reconsTR = transforms.Resize((imsize,imsize), antialias=True)(reconsTR).float().numpy().tolist()\n",
    "        return reconsTR, clipvoxelsTR\n",
    "    \n",
    "def batchwise_cosine_similarity(Z,B):\n",
    "    Z = Z.flatten(1)\n",
    "    B = B.flatten(1).T\n",
    "    Z_norm = torch.linalg.norm(Z, dim=1, keepdim=True)  # Size (n, 1).\n",
    "    B_norm = torch.linalg.norm(B, dim=0, keepdim=True)  # Size (1, b).\n",
    "    cosine_similarity = ((Z @ B) / (Z_norm @ B_norm)).T\n",
    "    return cosine_similarity\n",
    "\n",
    "def get_top_retrievals(clipvoxel, all_images, total_retrievals = 1):\n",
    "    '''\n",
    "    clipvoxel: output from do_recons that contains that information needed for retrievals\n",
    "    all_images: all ground truth actually seen images by the participant in day 2 run 1\n",
    "\n",
    "    outputs the top retrievals\n",
    "    '''\n",
    "    values_dict = {}\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        emb = clip_img_embedder(torch.reshape(all_images,(all_images.shape[0], 3, 224, 224)).to(device)).float() # CLIP-Image\n",
    "        emb = emb.cpu()\n",
    "        emb_ = clipvoxel # CLIP-Brain\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = np.reshape(emb_, (1, 425984))\n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "        emb_ = emb_.float()\n",
    "        fwd_sim = batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "        print(\"Given Brain embedding, find correct Image embedding\")\n",
    "    fwd_sim = np.array(fwd_sim.cpu())\n",
    "    which = np.flip(np.argsort(fwd_sim, axis = 0))\n",
    "    imsize = 224\n",
    "    for attempt in range(total_retrievals):\n",
    "        values_dict[f\"attempt{(attempt+1)}\"] = transforms.Resize((imsize,imsize), antialias=True)(all_images[which[attempt].copy()]).float().numpy().tolist()\n",
    "    return values_dict\n",
    "\n",
    "\n",
    "def convert_image_array_to_PIL(image_array):\n",
    "    if image_array.ndim == 4:\n",
    "        image_array = image_array[0]\n",
    "\n",
    "    # get the dimension to h, w, 3|1\n",
    "    if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "        image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "    \n",
    "    # clip the image array to 0-1\n",
    "    image_array = np.clip(image_array, 0, 1)\n",
    "    # convert the image array to uint8\n",
    "    image_array = (image_array * 255).astype('uint8')\n",
    "    # convert the image array to PIL\n",
    "    return Image.fromarray(image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e595bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils_glm import load_glmsingle_hrf_library, hrf_i_factory, fit_and_run_glm\n",
    "# BASE_TIME, GLMS_HRFS = load_glmsingle_hrf_library(f\"{data_path}/getcanonicalhrflibrary.tsv\")\n",
    "# hrf_fns = [hrf_i_factory(i, BASE_TIME, GLMS_HRFS) for i in range(1, 21)]\n",
    "# hrf_indices = np.load(f\"{data_path}/avg_hrfs_s1_s2_full.npy\").astype(int)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff1807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images=True\n",
    "save_individual_images=False\n",
    "only_betas = False  # skip plotting and saving, only calculate betas (located in all_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "211a4d40-643d-493b-874b-2030490b9bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 started\n",
      "TR 0\n",
      "blank\n",
      "TR 1\n",
      "blank\n",
      "TR 2\n",
      "blank\n",
      "TR 3\n",
      "blank\n",
      "TR 4\n",
      "blank\n",
      "TR 5\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_2458_cocoid_39454.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/conf/.venv/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n",
      "/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/conf/.venv/lib/python3.11/site-packages/nilearn/glm/_utils.py:205: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.where(X <= 0, 0, 1.0 / X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 6\n",
      "blank\n",
      "TR 7\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_2682_cocoid_72811.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ri4541@pu.win.princeton.edu/rtcloud-projects/mindeye/conf/.venv/lib/python3.11/site-packages/nilearn/glm/model.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sd = np.sqrt(self.vcov(matrix=matrix, dispersion=dispersion))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 8\n",
      "blank\n",
      "TR 9\n",
      "blank\n",
      "TR 10\n",
      "all_stimuli/unchosen_nsd_1000_images/unchosen_6050_cocoid_54489.png\n",
      "TR 11\n",
      "blank\n",
      "TR 12\n",
      "blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc_dir = f\"{derivatives_path}/motion_corrected\"\n",
    "mc_resampled_dir = f\"{derivatives_path}/motion_corrected_resampled\"\n",
    "if os.path.exists(mc_dir):\n",
    "    shutil.rmtree(mc_dir)\n",
    "os.makedirs(mc_dir)\n",
    "if os.path.exists(mc_resampled_dir):\n",
    "    shutil.rmtree(mc_resampled_dir)\n",
    "os.makedirs(mc_resampled_dir)\n",
    "\n",
    "rt_to_fmriprep_mat = f'{derivatives_path}/rt_to_fmriprep_mat'\n",
    "# set the output type to NIFTI_GZ\n",
    "os.environ['FSLOUTPUTTYPE'] = 'NIFTI_GZ'\n",
    "assert np.all(fmriprep_boldref_nib.affine == union_mask_img.affine)\n",
    "all_betas = []\n",
    "\n",
    "# Loop over all 11 runs in the session\n",
    "n_runs = 11\n",
    "for run_num in range(1, n_runs + 1):\n",
    "    print(f\"Run {run_num} started\")\n",
    "    mc_params = []\n",
    "    imgs = []\n",
    "    events_df = ndscore_events[run_num - 1]\n",
    "    tr_labels_hrf = ndscore_tr_labels[run_num - 1][\"tr_label_hrf\"].tolist()\n",
    "    events_df = events_df[events_df['image_name'] != 'blank.jpg']  # must drop blank.jpg after tr_labels_hrf is defined to keep indexing consistent\n",
    "    beta_maps_list = []\n",
    "    all_trial_names_list = []\n",
    "    all_images = None\n",
    "\n",
    "    # define save_path\n",
    "    save_path = f\"{derivatives_path}/{sub}_{session}_task-{func_task_name}_run-{run_num:02d}_recons\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    save_individual_images = True\n",
    "    if save_individual_images:\n",
    "        os.makedirs(os.path.join(save_path, \"individual_images\"), exist_ok=True)\n",
    "\n",
    "    all_recons_save = []\n",
    "    all_clipvoxels_save = []\n",
    "    all_ground_truth_save = []\n",
    "    all_retrieved_save = []\n",
    "\n",
    "    stimulus_trial_counter = 0\n",
    "    plot_images = True\n",
    "    T1_brain = f\"{data_path}/sub-005_desc-preproc_T1w_brain.nii.gz\"\n",
    "    n_trs = 192\n",
    "    assert len(tr_labels_hrf) == n_trs, \"there should be image labels for each TR\"\n",
    "    assert all(label in image_names for label in tr_labels_hrf if label != 'blank'), \"Some labels in tr_labels_hrf are missing from image_names.\"\n",
    "    assert len(images) > n_trs, \"images array is too short.\"\n",
    "    for TR in range(n_trs-1):\n",
    "        print(f\"TR {TR}\")\n",
    "        # load in the nifti volume\n",
    "        cur_vol = f\"{sub}_{session}_task-{func_task_name}_run-{run_num:02d}_bold_{TR:04d}\"\n",
    "        curr_image_path = os.path.join(single_vols_path, f\"{cur_vol}.nii.gz\")\n",
    "        image_data = nib.load(curr_image_path)\n",
    "        current_label = tr_labels_hrf[TR]\n",
    "        print(current_label)\n",
    "        if TR == 0 and run_num == 1:\n",
    "            nib.save(image_data, rt_vol0)  # save this volume to mcflirt to for subsequent volumes\n",
    "\n",
    "            os.system(f\"flirt -in {rt_vol0} \\\n",
    "                -ref {fmriprep_boldref} \\\n",
    "                -omat {rt_to_fmriprep_mat} \\\n",
    "                -dof 6\")  # register rt_vol0 to the functional boldref image from the prev session\n",
    "\n",
    "        start = time.time()\n",
    "        mc = f\"{mc_dir}/{cur_vol}_mc\"\n",
    "        os.system(f\"{fsl_path}/mcflirt -in {os.path.join(single_vols_path, f'{cur_vol}.nii.gz')} -reffile {os.path.join(single_vols_path, f'{sub}_{session}_task-{func_task_name}_run-01_bold_0000.nii.gz')} -out {mc} -plots -mats\")\n",
    "        mc_params.append(np.loadtxt(f'{mc}.par'))\n",
    "\n",
    "        current_tr_to_ses1 = f\"{derivatives_path}/current_tr_to_ses1_run{run_num}\"\n",
    "        os.system(f\"convert_xfm -concat {rt_to_fmriprep_mat} -omat {current_tr_to_ses1} {mc}.mat/MAT_0000\")    \n",
    "        \n",
    "        # apply concatenated matrix to the current TR\n",
    "        final_vol = f\"{mc_resampled_dir}/{session}_run-{run_num:02d}_{TR:04d}_mc_boldres.nii.gz\"\n",
    "        os.system(f\"flirt -in {curr_image_path} \\\n",
    "            -ref {fmriprep_boldref} \\\n",
    "            -out {final_vol} \\\n",
    "            -init {current_tr_to_ses1} \\\n",
    "            -applyxfm\")\n",
    "\n",
    "        os.system(f\"rm -r {mc}.mat\")\n",
    "        imgs.append(get_data(final_vol))\n",
    "        \n",
    "        if current_label not in ('blank', 'blank.jpg'):\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] = events_df['onset'].astype(float)\n",
    "\n",
    "            run_start_time = events_df['onset'].iloc[0]\n",
    "            events_df = events_df.copy()\n",
    "            events_df['onset'] -= run_start_time\n",
    "\n",
    "            cropped_events = events_df[events_df.onset <= TR*tr_length]\n",
    "            cropped_events = cropped_events.copy()\n",
    "            cropped_events.loc[:, 'trial_type'] = np.where(cropped_events['trial_number'] == stimulus_trial_counter, \"probe\", \"reference\")\n",
    "            cropped_events = cropped_events.drop(columns=['is_correct', 'image_name', 'response_time', 'trial_number'])\n",
    "\n",
    "            # collect all of the images at each TR into a 4D time series\n",
    "            img = np.rollaxis(np.array(imgs),0,4)\n",
    "            img = new_img_like(fmriprep_boldref_nib,img,copy_header=True)\n",
    "            # run the model with mc_params confounds to motion correct\n",
    "            lss_glm = FirstLevelModel(t_r=tr_length,slice_time_ref=0,hrf_model='glover',\n",
    "                        drift_model='cosine', drift_order=1,high_pass=0.01,mask_img=union_mask_img,\n",
    "                        signal_scaling=False,smoothing_fwhm=None,noise_model='ar1',\n",
    "                        n_jobs=-1,verbose=-1,memory_level=1,minimize_memory=True)\n",
    "\n",
    "            lss_glm.fit(run_imgs=img, events=cropped_events, confounds = pd.DataFrame(np.array(mc_params)))\n",
    "            dm = lss_glm.design_matrices_[0]\n",
    "            # get the beta map and mask it\n",
    "            beta_map = lss_glm.compute_contrast(\"probe\", output_type=\"effect_size\")\n",
    "            beta_map_np = beta_map.get_fdata()\n",
    "            beta_map_np = fast_apply_mask(target=beta_map_np,mask=union_mask_img.get_fdata())\n",
    "            all_betas.append(beta_map_np)\n",
    "            \n",
    "            if only_betas:\n",
    "                continue\n",
    "            if \"MST_pairs\" in current_label:\n",
    "                correct_image_index = np.where(current_label == vox_image_names)[0][0]  # using the first occurrence based on image name, assumes that repeated images are identical (which they should be)\n",
    "                z_mean = np.mean(np.array(all_betas), axis=0)\n",
    "                z_std = np.std(np.array(all_betas), axis=0)\n",
    "                betas = ((np.array(all_betas) - z_mean) / (z_std + 1e-6))[-1]  # use only the beta pattern from the most recent image\n",
    "                betas = betas[np.newaxis, np.newaxis, :]\n",
    "                betas_tt = torch.Tensor(betas).to(\"cpu\")\n",
    "                reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "                if clipvoxelsTR is None:\n",
    "                    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                        voxel = betas_tt\n",
    "                        voxel = voxel.to(device)\n",
    "                        assert voxel.shape[1] == 1\n",
    "                        voxel_ridge = model.ridge(voxel[:,[-1]],0) # 0th index of subj_list\n",
    "                        backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                        blurry_image_enc = blurry_image_enc0[0]\n",
    "                        clipvoxelsTR = clip_voxels.cpu()\n",
    "                values_dict = get_top_retrievals(clipvoxelsTR, all_images=images[MST_idx], total_retrievals=5)\n",
    "                image_array = np.array(reconsTR)[0]\n",
    "                # If the image has 3 channels (RGB), you need to reorder the dimensions\n",
    "                if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "                    image_array = np.transpose(image_array, (1, 2, 0))  # Change shape to (height, width, 3)\n",
    "\n",
    "                # Display the image\n",
    "                if plot_images:\n",
    "                    # plot original and reconstructed images\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.title(\"Reconstructed Image\")\n",
    "                    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    # plot original with top 5 retrievals\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.subplot(1, 6, 1)\n",
    "                    plt.title(\"Original Image\")\n",
    "                    plt.imshow(images[correct_image_index].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    for i in range(5):\n",
    "                        plt.subplot(1, 6, i+2)\n",
    "                        plt.title(f\"Retrieval {i+1}\")\n",
    "                        plt.imshow(np.array(values_dict[f\"attempt{i+1}\"][0]).transpose(1, 2, 0), cmap='gray')\n",
    "                        plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                # save reconstructed image, retrieved images, clip_voxels, and ground truth image\n",
    "                if save_individual_images:\n",
    "                    # save the reconstructed image\n",
    "                    convert_image_array_to_PIL(image_array).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_reconstructed.png\"))\n",
    "                    # save the retrieved images\n",
    "                    for key, value in values_dict.items():\n",
    "                        if (not ('ground_truth' in key)):\n",
    "                            convert_image_array_to_PIL(np.array(value)).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_retrieved_{key}.png\"))\n",
    "                    # save the clip_voxels\n",
    "                    np.save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_clip_voxels.npy\"), clipvoxelsTR)\n",
    "                    # save the ground truth image\n",
    "                    convert_image_array_to_PIL(images[correct_image_index].numpy()).save(os.path.join(save_path, \"individual_images\", f\"run{run_num}_TR{TR}_ground_truth.png\"))\n",
    "                all_recons_save.append(image_array)\n",
    "                all_clipvoxels_save.append(clipvoxelsTR)\n",
    "                all_ground_truth_save.append(images[correct_image_index].numpy())\n",
    "                all_retrieved_save.append([np.array(value) for key, value in values_dict.items() if (not ('ground_truth' in key))])\n",
    "            else:\n",
    "                pass\n",
    "            stimulus_trial_counter += 1\n",
    "        elif current_label == 'blank.jpg':\n",
    "            stimulus_trial_counter += 1\n",
    "        else:\n",
    "            assert current_label == 'blank'\n",
    "\n",
    "    # save betas so far\n",
    "    np.save(os.path.join(save_path, f\"betas_run-{run_num:02d}.npy\"), np.array(all_betas))\n",
    "    if only_betas:\n",
    "        continue\n",
    "    # save the design matrix for the current run\n",
    "    dm.to_csv(os.path.join(save_path, f\"design_run-{run_num:02d}.csv\"))\n",
    "    plot_design_matrix(dm, output_file=os.path.join(save_path, \"dm\"))\n",
    "    dm[['probe', 'reference']].plot(title='Probe/Reference Regressors', figsize=(10, 4))\n",
    "    # dm[['probe_hrf_callable', 'reference_hrf_callable']].plot(title='Probe/Reference Regressors', figsize=(10, 4))\n",
    "    plt.savefig(os.path.join(save_path, \"regressors\"))\n",
    "    print(f\"==END OF RUN {run_num}!==\\n\")\n",
    "    # save the tensors\n",
    "    if all_recons_save:\n",
    "        all_recons_save_tensor = torch.tensor(all_recons_save).permute(0,3,1,2)\n",
    "        all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save, dim=0)\n",
    "        all_ground_truth_save_tensor = torch.tensor(all_ground_truth_save)\n",
    "        all_retrieved_save_tensor = torch.stack([torch.tensor(np.array(item)) for item in all_retrieved_save], dim=0)\n",
    "        torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "        torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "        torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "        torch.save(all_retrieved_save_tensor, os.path.join(save_path, \"all_retrieved.pt\"))\n",
    "        print(\"all_recons_save_tensor.shape: \", all_recons_save_tensor.shape)\n",
    "        print(\"all_clipvoxels_save_tensor.shape: \", all_clipvoxels_save_tensor.shape)\n",
    "        print(\"all_ground_truth_save_tensor.shape: \", all_ground_truth_save_tensor.shape)\n",
    "        print(\"all_retrieved_save_tensor.shape: \", all_retrieved_save_tensor.shape)\n",
    "        print(\"All tensors saved successfully on \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec54d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display all recons with originals\n",
    "# all_recons_save = torch.load(os.path.join(save_path, \"all_recons.pt\"))\n",
    "# for i in range(len(all_recons_save)):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.title(\"Reconstructed Image\")\n",
    "#     plt.imshow(all_recons_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load and display top 5 retrievals and originals\n",
    "# for i in range(len(all_retrieved_save)):\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.subplot(1, 6, 1)\n",
    "#     plt.title(\"Original\")\n",
    "#     plt.imshow(all_ground_truth_save[i].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#     plt.axis('off')  # Hide axes\n",
    "#     for j in range(5):\n",
    "#         plt.subplot(1, 6, j+2)\n",
    "#         plt.title(f\"Top {j+1}\")\n",
    "#         plt.imshow(all_retrieved_save[i][j][0].numpy().transpose(1, 2, 0), cmap='gray')\n",
    "#         plt.axis('off')  # Hide axes\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_recons_and_evaluate(\n",
    "#     beta_series,  # shape: (n_conditions, n_voxels)\n",
    "#     images,       # tensor of all images, shape: (n_images, 3, 224, 224)\n",
    "#     model,        # loaded model\n",
    "#     do_reconstructions,  # function for recon\n",
    "#     device,       # torch device\n",
    "#     save_path=None,    # directory to save results\n",
    "#     metrics_module=None,  # module or dict with metric functions, optional\n",
    "#     save_results=False,   # flag to control saving, defaults to False\n",
    "#     test_idx=None,         # indices for test set, optional\n",
    "#     do_zscore=False\n",
    "# ):\n",
    "#     if test_idx is not None:\n",
    "#         beta_series = beta_series[test_idx]\n",
    "#         images = images[test_idx]\n",
    "\n",
    "#     all_recons_save_tensor = []\n",
    "#     all_clipvoxels_save_tensor = []\n",
    "#     all_ground_truth_save_tensor = []\n",
    "\n",
    "#     for idx in range(beta_series.shape[0]):\n",
    "#         beta_pattern = beta_series[np.newaxis, np.newaxis, idx, :]  # (1,1,n_vox)\n",
    "#         if do_zscore:\n",
    "#             # Z-score using only betas up to and including the current image\n",
    "#             beta_series_up_to_now = beta_series[:idx+1]\n",
    "#             beta_pattern = utils_mindeye.zscore(beta_pattern, beta_series_up_to_now)\n",
    "\n",
    "#         betas_tt = torch.Tensor(beta_pattern).to(\"cpu\")\n",
    "#         reconsTR, clipvoxelsTR = do_reconstructions(betas_tt)\n",
    "#         if clipvoxelsTR is None:\n",
    "#             with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "#                 voxel = betas_tt.to(device)\n",
    "#                 assert voxel.shape[1] == 1\n",
    "#                 voxel_ridge = model.ridge(voxel[:, [-1]], 0)\n",
    "#                 backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "#                 clipvoxelsTR = clip_voxels0.cpu()\n",
    "\n",
    "#         image_array = np.array(reconsTR)[0]\n",
    "#         if image_array.ndim == 3 and image_array.shape[0] == 3:\n",
    "#             image_array = np.transpose(image_array, (1, 2, 0))\n",
    "\n",
    "#         all_recons_save_tensor.append(reconsTR)\n",
    "#         all_clipvoxels_save_tensor.append(clipvoxelsTR)\n",
    "#         all_ground_truth_save_tensor.append(images[idx])\n",
    "\n",
    "#     all_recons_save_tensor = torch.stack(all_recons_save_tensor)\n",
    "#     all_clipvoxels_save_tensor = torch.stack(all_clipvoxels_save_tensor)\n",
    "#     all_ground_truth_save_tensor = torch.stack(all_ground_truth_save_tensor)\n",
    "\n",
    "#     if save_results and save_path is not None:\n",
    "#         os.makedirs(save_path, exist_ok=True)\n",
    "#         torch.save(all_recons_save_tensor, os.path.join(save_path, \"all_recons.pt\"))\n",
    "#         torch.save(all_clipvoxels_save_tensor, os.path.join(save_path, \"all_clipvoxels.pt\"))\n",
    "#         torch.save(all_ground_truth_save_tensor, os.path.join(save_path, \"all_ground_truth.pt\"))\n",
    "\n",
    "#     if metrics_module is not None:\n",
    "#         with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "#             unique_clip_voxels = all_clipvoxels_save_tensor\n",
    "#             unique_ground_truth = all_ground_truth_save_tensor\n",
    "#             all_fwd_acc, all_bwd_acc = metrics_module['calculate_retrieval_metrics'](unique_clip_voxels, unique_ground_truth)\n",
    "#             pixcorr = metrics_module['calculate_pixcorr'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             ssim_ = metrics_module['calculate_ssim'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             alexnet2, alexnet5 = metrics_module['calculate_alexnet'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             inception = metrics_module['calculate_inception_v3'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             clip_ = metrics_module['calculate_clip'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             efficientnet = metrics_module['calculate_efficientnet_b1'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "#             swav = metrics_module['calculate_swav'](all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "#         df_metrics = pd.DataFrame({\n",
    "#             \"Metric\": [\n",
    "#                 \"alexnet2\",\n",
    "#                 \"alexnet5\",\n",
    "#                 \"inception\",\n",
    "#                 \"clip_\",\n",
    "#                 \"efficientnet\",\n",
    "#                 \"swav\",\n",
    "#                 \"pixcorr\",\n",
    "#                 \"ssim\",\n",
    "#                 \"all_fwd_acc\",\n",
    "#                 \"all_bwd_acc\"\n",
    "#             ],\n",
    "#             \"Value\": [\n",
    "#                 alexnet2,\n",
    "#                 alexnet5,\n",
    "#                 inception,\n",
    "#                 clip_,\n",
    "#                 efficientnet,\n",
    "#                 swav,\n",
    "#                 pixcorr,\n",
    "#                 ssim_,\n",
    "#                 all_fwd_acc,\n",
    "#                 all_bwd_acc\n",
    "#             ]\n",
    "#         })\n",
    "#         df_metrics.set_index(\"Metric\", inplace=True)\n",
    "#         if save_results and save_path is not None:\n",
    "#             df_metrics.to_csv(os.path.join(save_path, \"metrics.csv\"))\n",
    "#         print(df_metrics)\n",
    "\n",
    "#     return all_recons_save_tensor, all_clipvoxels_save_tensor, all_ground_truth_save_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6675825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation metrics\n",
    "from utils_mindeye import calculate_retrieval_metrics, calculate_alexnet, calculate_clip, calculate_swav, calculate_efficientnet_b1, calculate_inception_v3, calculate_pixcorr, calculate_ssim, deduplicate_tensors\n",
    "all_recons_save_tensor = []\n",
    "all_clipvoxels_save_tensor = []\n",
    "all_ground_truth_save_tensor = []\n",
    "all_retrieved_save_tensor = []\n",
    "\n",
    "for run_num in range(n_runs):\n",
    "    save_path = f\"{derivatives_path}/{sub}_{session}_task-{func_task_name}_run-{run_num+1:02d}_recons\"\n",
    "\n",
    "    try:\n",
    "        # recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16)\n",
    "        # clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16)\n",
    "        # ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16)\n",
    "        recons = torch.load(os.path.join(save_path, \"all_recons.pt\")).to(torch.float16).to(device)\n",
    "        clipvoxels = torch.load(os.path.join(save_path, \"all_clipvoxels.pt\")).to(torch.float16).to(device)\n",
    "        ground_truth = torch.load(os.path.join(save_path, \"all_ground_truth.pt\")).to(torch.float16).to(device)\n",
    "\n",
    "        all_recons_save_tensor.append(recons)\n",
    "        all_clipvoxels_save_tensor.append(clipvoxels)\n",
    "        all_ground_truth_save_tensor.append(ground_truth)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Tensors not found. Please check the save path.\")\n",
    "\n",
    "# Concatenate tensors along the first dimension\n",
    "try:\n",
    "    all_recons_save_tensor = torch.cat(all_recons_save_tensor, dim=0)\n",
    "    all_clipvoxels_save_tensor = torch.cat(all_clipvoxels_save_tensor, dim=0)\n",
    "    all_ground_truth_save_tensor = torch.cat(all_ground_truth_save_tensor, dim=0)\n",
    "except RuntimeError:\n",
    "    print('Error: Couldn\\'t concatenate tensors')\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    unique_clip_voxels, unique_ground_truth, duplicated = deduplicate_tensors(all_clipvoxels_save_tensor, all_ground_truth_save_tensor)\n",
    "    \n",
    "    print('calculating retrieval subset 0 (first set of repeats)')\n",
    "    unique_clip_voxels_subset0 = all_clipvoxels_save_tensor[np.array(duplicated)[:,0]]\n",
    "    unique_ground_truth_subset0 = all_ground_truth_save_tensor[np.array(duplicated)[:,0]]\n",
    "    all_fwd_acc_subset0, all_bwd_acc_subset0 = calculate_retrieval_metrics(unique_clip_voxels_subset0, unique_ground_truth_subset0)\n",
    "\n",
    "    print('calculating retrieval subset 1 (second set of repeats)')\n",
    "    unique_clip_voxels_subset1 = all_clipvoxels_save_tensor[np.array(duplicated)[:,1]]\n",
    "    unique_ground_truth_subset1 = all_ground_truth_save_tensor[np.array(duplicated)[:,1]]\n",
    "    all_fwd_acc_subset1, all_bwd_acc_subset1 = calculate_retrieval_metrics(unique_clip_voxels_subset1, unique_ground_truth_subset1)\n",
    "    pixcorr = calculate_pixcorr(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    ssim_ = calculate_ssim(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    alexnet2, alexnet5 = calculate_alexnet(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    inception = calculate_inception_v3(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    clip_ = calculate_clip(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    efficientnet = calculate_efficientnet_b1(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "    swav = calculate_swav(all_recons_save_tensor, all_ground_truth_save_tensor)\n",
    "\n",
    "\n",
    "# save the results to a csv file\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"alexnet2\",\n",
    "        \"alexnet5\",\n",
    "        \"inception\",\n",
    "        \"clip_\",\n",
    "        \"efficientnet\",\n",
    "        \"swav\",\n",
    "        \"pixcorr\",\n",
    "        \"ssim\",\n",
    "        \"all_fwd_acc_subset0\",\n",
    "        \"all_bwd_acc_subset0\",\n",
    "        \"all_fwd_acc_subset1\",\n",
    "        \"all_bwd_acc_subset1\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        alexnet2,\n",
    "        alexnet5,\n",
    "        inception,\n",
    "        clip_,\n",
    "        efficientnet,\n",
    "        swav,\n",
    "        pixcorr,\n",
    "        ssim_,\n",
    "        all_fwd_acc_subset0,\n",
    "        all_bwd_acc_subset0,\n",
    "        all_fwd_acc_subset1,\n",
    "        all_bwd_acc_subset1\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e71597",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_metrics = [\"alexnet2\", \"alexnet5\", \"inception\", \"clip_\", \"all_fwd_acc_subset0\", \"all_bwd_acc_subset0\", \"all_bwd_acc_subset1\", \"all_fwd_acc_subset1\"]\n",
    "lower_better_metrics = [\"efficientnet\", \"swav\"]\n",
    "higher_better_arrow = \"↑\"\n",
    "lower_better_arrow = \"↓\"\n",
    "\n",
    "# Format function\n",
    "def format_metric(metric, value):\n",
    "    if metric in percentage_metrics:\n",
    "        return f\"{value * 100:.2f}% {higher_better_arrow}\"\n",
    "    elif metric in lower_better_metrics:\n",
    "        return f\"{value:.2f} {lower_better_arrow}\"\n",
    "    else:\n",
    "        return f\"{value:.2f} {higher_better_arrow}\"\n",
    "\n",
    "# Apply formatting\n",
    "df_formatted = df_metrics.copy()\n",
    "df_formatted[\"Formatted\"] = df_formatted.apply(lambda row: format_metric(row[\"Metric\"], row[\"Value\"]), axis=1)\n",
    "df_formatted.set_index(\"Metric\", inplace=True)\n",
    "df_formatted.index.name = \"Metric\"\n",
    "\n",
    "# Print and save\n",
    "display(df_formatted[[\"Formatted\"]])\n",
    "\n",
    "df_formatted[[\"Formatted\"]].to_csv(os.path.join(save_path, \"metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = df_metrics.copy()\n",
    "df_formatted[\"Formatted\"] = df_formatted.apply(lambda row: format_metric(row[\"Metric\"], row[\"Value\"]), axis=1)\n",
    "\n",
    "# Print and save\n",
    "# display(df_formatted[[\"Formatted\"]])\n",
    "display(df_formatted[['Metric', 'Formatted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238af6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
